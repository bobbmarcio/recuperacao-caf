# ğŸ—œï¸ Guia para Dumps Grandes e Comprimidos (GZIP)

## ï¿½ PROBLEMA IDENTIFICADO

O arquivo `dump-caf_mapa-20250301-202506151151.sql` tem extensÃ£o `.sql` mas Ã© **comprimido com gzip**!

### âŒ Erro Original:
```
psql: error: invalid command \ï¿½ï¿½ï¿½ï¿½ï¿½lï¿½cï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Oï¿½ï¿½-kï¿½`ï¿½ï¿½ï¿½`Kï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½omi
```

### âœ… Causa:
Tentativa de importar arquivo binÃ¡rio (gzip) como texto SQL.

## ï¿½ Como Identificar Arquivo Comprimido

```powershell
# Verificar primeiros bytes
(Get-Content "dumps\dump-caf_mapa-20250301-202506151151.sql" -TotalCount 3 -Encoding Byte | ForEach-Object { [System.Convert]::ToString($_, 16).PadLeft(2, '0') }) -join ' '

# Resultado: "1f 8b 08" = GZIP comprimido âœ…
# Se fosse texto SQL normal veria caracteres legÃ­veis
```

## âœ… SOLUÃ‡ÃƒO: Comando Correto

### MÃ©todo Recomendado - ImportaÃ§Ã£o Direta:
```powershell
# 1. Criar schema
docker exec postgres-caf-dumps psql -U caf_user -d caf_analysis -c "CREATE SCHEMA IF NOT EXISTS caf_20250301;"

# 2. Importar descomprimindo na hora
docker exec postgres-caf-dumps sh -c "
export PGPASSWORD='caf_password123'
gunzip -c /dumps/dump-caf_mapa-20250301-202506151151.sql | \
grep -v 'SET transaction_timeout' | \
sed 's/public\./caf_20250301\./g; s/SCHEMA public/SCHEMA caf_20250301/g' | \
psql -U caf_user -d caf_analysis -v ON_ERROR_STOP=1 --single-transaction
"
```

### MÃ©todo Alternativo - Descomprimir Primeiro:
```powershell
# Se preferir descomprimir primeiro (usa mais espaÃ§o)
docker exec postgres-caf-dumps sh -c "gunzip -c /dumps/dump-caf_mapa-20250301-202506151151.sql > /tmp/dump_descomprimido.sql"

# Depois importar
docker exec postgres-caf-dumps psql -U caf_user -d caf_analysis -c "SET search_path TO caf_20250301, public;" -f /tmp/dump_descomprimido.sql

# Limpar temporÃ¡rio
docker exec postgres-caf-dumps rm /tmp/dump_descomprimido.sql
```

#### Windows:
```bash
# OpÃ§Ã£o 1: Download oficial
https://www.postgresql.org/download/windows/

# OpÃ§Ã£o 2: Via Chocolatey
choco install postgresql

# OpÃ§Ã£o 3: Via Scoop
scoop install postgresql
```

#### Via Docker (Recomendado):
```bash
# Subir PostgreSQL temporÃ¡rio
docker run -d --name postgres-temp \
  -e POSTGRES_PASSWORD=temp123 \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_DB=postgres \
  -p 5432:5432 \
  postgres:15

# Verificar se estÃ¡ rodando
docker ps
```

### 2. Configurar Credenciais

Edite o arquivo `.env`:

```env
# PostgreSQL para processamento de dumps grandes
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=postgres
POSTGRES_PASSWORD=temp123
POSTGRES_DATABASE=postgres

# MongoDB (jÃ¡ configurado)
MONGODB_CONNECTION_STRING=mongodb://app_user:app_password@localhost:27017/audit_db?authSource=audit_db
MONGODB_DATABASE=audit_db
MONGODB_COLLECTION=data_changes
```

### 3. Testar ConfiguraÃ§Ã£o

```bash
# Testar PostgreSQL
python test_large_dump.py

# Se conectar corretamente, testar processamento
python src/main.py analyze --config config/monitoring_config.yaml --dump-dir dumps/
```

## ğŸš€ Como Funciona o Processamento de Dumps Grandes

### Fluxo AutomÃ¡tico:
1. **DetecÃ§Ã£o**: Sistema verifica tamanho total dos dumps
2. **DecisÃ£o**: Se > 2GB â†’ usa PostgreSQL, senÃ£o â†’ usa memÃ³ria
3. **Processamento**:
   - Cria bancos temporÃ¡rios para cada dump
   - Restaura dumps usando `psql`/`pg_restore`
   - Compara dados via SQL otimizado
   - Remove bancos temporÃ¡rios
4. **Auditoria**: Insere alteraÃ§Ãµes no MongoDB

### Estimativas de Performance:
- **RestauraÃ§Ã£o**: ~120 segundos por GB
- **ComparaÃ§Ã£o**: ~30 segundos por GB
- **Total para 5GB**: ~12.5 minutos

### Exemplo de ExecuÃ§Ã£o:
```bash
# Dump de 5.2GB
ğŸ“ Encontrados 2 dumps para anÃ¡lise
ğŸ“Š Tamanho total: 5.20 GB
ğŸ”„ Usando estratÃ©gia PostgreSQL para arquivos grandes...
â±ï¸  Tempo estimado: 12.8 minutos
ğŸ“ Detectadas 1,245 alteraÃ§Ãµes
âœ… AlteraÃ§Ãµes inseridas no MongoDB
ğŸ‰ AnÃ¡lise concluÃ­da!
```

## ğŸ¯ Vantagens da EstratÃ©gia PostgreSQL

### Performance:
- âœ… Processa 5-6GB em ~15 minutos
- âœ… Usa comparaÃ§Ã£o SQL nativa (muito rÃ¡pida)
- âœ… NÃ£o consome RAM excessiva

### Escalabilidade:
- âœ… Suporta dumps de qualquer tamanho
- âœ… Processamento em paralelo de tabelas
- âœ… Limpeza automÃ¡tica de recursos

### PrecisÃ£o:
- âœ… Parsing nativo do PostgreSQL
- âœ… ComparaÃ§Ã£o exata com `IS DISTINCT FROM`
- âœ… Suporte a todos os tipos de dados

## ğŸ”§ Comandos Ãšteis

### AnÃ¡lise Completa:
```bash
# Processar todos os dumps
python src/main.py analyze --config config/monitoring_config.yaml --dump-dir dumps/

# Com logs detalhados
python src/main.py --debug analyze --config config/monitoring_config.yaml --dump-dir dumps/
```

### Consultas:
```bash
# Ver todas as alteraÃ§Ãµes
python src/main.py query --limit 50

# Filtrar por tabela
python src/main.py query --table usuarios --limit 20

# Filtrar por coluna
python src/main.py query --table usuarios --column email --limit 10
```

### Docker PostgreSQL:
```bash
# Subir PostgreSQL temporÃ¡rio
docker run -d --name postgres-temp -e POSTGRES_PASSWORD=temp123 -p 5432:5432 postgres:15

# Parar e remover
docker stop postgres-temp && docker rm postgres-temp
```

## ğŸ“Š Monitoramento de Progresso

Durante o processamento de dumps grandes, vocÃª verÃ¡:

```
ğŸ”§ Criando banco temporÃ¡rio: temp_dump_0_1704123456
ğŸ“ Dump restaurado com sucesso no banco temp_dump_0_1704123456
ğŸ” Comparando dump_inicial.sql â†’ dump_alteracoes.sql
ğŸ“Š Tabela usuarios: 834 alteraÃ§Ãµes detectadas
ğŸ“Š Tabela produtos: 411 alteraÃ§Ãµes detectadas
ğŸ§¹ Limpando bancos temporÃ¡rios...
âœ… Processamento concluÃ­do: 1,245 alteraÃ§Ãµes detectadas
```

## âš ï¸ ConsideraÃ§Ãµes Importantes

### Recursos:
- **Disco**: Bancos temporÃ¡rios ocupam ~2x o tamanho do dump
- **CPU**: Processo intensivo durante restauraÃ§Ã£o
- **Rede**: Se PostgreSQL nÃ£o for local

### SeguranÃ§a:
- Bancos temporÃ¡rios sÃ£o automaticamente removidos
- Credenciais ficam apenas no `.env` local
- Dados nÃ£o ficam persistidos no PostgreSQL

### Fallback:
- Se PostgreSQL nÃ£o estiver disponÃ­vel, sistema usa estratÃ©gia em memÃ³ria
- Aviso automÃ¡tico quando dumps sÃ£o muito grandes para memÃ³ria

---

ğŸ‰ **Pronto!** Seu sistema agora pode processar dumps de qualquer tamanho eficientemente!

## ğŸ¤– Scripts Automatizados para ImportaÃ§Ã£o em Lote

Criamos scripts que automatizam a importaÃ§Ã£o de **todos** os dumps CAF encontrados em `./dumps`:

### Script Python (Recomendado):
```bash
# Importar todos os dumps CAF automaticamente
python import-all-caf-dumps.py

# Ver progresso detalhado
python import-all-caf-dumps.py --verbose
```

### Script PowerShell:
```powershell
# Importar todos os dumps CAF
.\import-all-dumps.ps1

# Importar pulando jÃ¡ existentes
.\import-all-dumps.ps1 -SkipExisting

# ForÃ§ar reimportaÃ§Ã£o de todos
.\import-all-dumps.ps1 -Force
```

### âœ… O que os Scripts Fazem Automaticamente:

1. **Detectam tipo de arquivo**: Gzip ou SQL texto
2. **Extraem data do nome**: `dump-caf_mapa-20250301-...` â†’ `2025-03-01`
3. **Criam schema por data**: `caf_20250301`, `caf_20250401`, etc.
4. **Filtram comandos problemÃ¡ticos**: `transaction_timeout`, etc.
5. **Ajustam schemas**: `public.` â†’ `"caf_20250301".`
6. **Registram metadados**: Tabela `dump_metadata` para auditoria
7. **Pulam jÃ¡ importados**: Evita reimportaÃ§Ãµes desnecessÃ¡rias

### ğŸ“Š Exemplo de ExecuÃ§Ã£o:
```
=== Iniciando importaÃ§Ã£o automÃ¡tica de dumps CAF ===
Encontrados 2 dumps CAF para processamento

--- Processando dump: dump-caf_mapa-20250301-202506151151.sql ---
Data: 2025-03-01, Schema: caf_20250301
Arquivo Ã© comprimido (gzip)
âœ… Schema 'caf_20250301' criado com sucesso
âœ… Dump importado com sucesso para schema 'caf_20250301'

--- Processando dump: dump-caf_mapa-20250401-202506161955.sql ---
Data: 2025-04-01, Schema: caf_20250401
Arquivo Ã© comprimido (gzip)
âœ… Schema 'caf_20250401' criado com sucesso
âœ… Dump importado com sucesso para schema 'caf_20250401'

=== Resumo da ImportaÃ§Ã£o ===
Dumps processados: 2
ImportaÃ§Ãµes bem-sucedidas: 2
ImportaÃ§Ãµes puladas (jÃ¡ existentes): 0
ImportaÃ§Ãµes falharam: 0
```

### ğŸ” Verificar Schemas Importados:
```sql
-- Listar todos os schemas CAF
SELECT schema_name FROM information_schema.schemata 
WHERE schema_name LIKE 'caf_%' ORDER BY schema_name;

-- Verificar metadados das importaÃ§Ãµes
SELECT * FROM dump_metadata ORDER BY import_timestamp DESC;

-- Contar tabelas por schema
SELECT 
    schemaname, 
    COUNT(*) as table_count 
FROM pg_tables 
WHERE schemaname LIKE 'caf_%' 
GROUP BY schemaname 
ORDER BY schemaname;
```

---
